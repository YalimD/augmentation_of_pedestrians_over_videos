FUTURE THESÄ°S TO DOS:

-first handle github, SAFELY BY FIRST BACKUP
-then clear the code from redundancy, bugs and improve style
- then complete the biography to have a proper literature review

Automatic Navigable Area Generation
- Instead of using meanshift, we may directly use the SLIC method's segmentation and take the average in lab color space to get if the area can be considered to be navigable
(The difference would be time)

-the prblem with slic, is that its segmentation doesn't give best results that can be used for mesh creation. It needs a lot of reconnection work, which is again time consuming
- But meanshift doesn't work well witht he shadows ( http://www.cit.iit.bas.bg/CIT_2013/v13-1/S_Murali,%20V_Govindan.pdf seems very promising)
-Also, meanshift requires cleaning of unwanted tiny clusters of either type of detection.
- It appears we have a tradeoff here, meanshift has higher quality yet takes long and hard to understand where SLIC is opposite in any other way. We might just let the user to select (default is meanshift)


-First, using the C/C++ version of the mean-shift algorithm, the area should be segmented in LAB color space.
We use LAB color space instead of RGB, because the distance between different colors are more linear in LAB.
	+ For this, we must first investigate the Mean-Shift algorithm in detail and implement it (in an optimized manner)
		C/C++. We may also adjust several parameters according to our needs. (This will require several experiements which needs to be reflected in the report)
	+ The MATLAB code for finding the LAB colors that are in a certain distance to the determined colors (where distance is also an experimental topic) is already written and commented

- The navigable areas must be determined by their color. There are two options for that
	- the color should be either selected by the user (like an option at the beginning of the framework)
	- or, the pixel data under the detection can be used to create a histogram where the top 3 (arbitrary amount, adjust according to performance)
	lab colors are selected. 

- THEN the area should be generated as a mesh, but there are some problems with that
	+ First, the silhouette of the navigable areas needs to be extracted (meanshift does that)
	+ then, the resulting polygons points should be extracted using OpenCV
	CAN't this be done by directly using it to create a polygon (needs research)
	+ the holes in the polygon should be got rid of too (virus algortihm ? seen in that paper including the guitar)
	+ another problem is finding the perspective of the area (in what angle we are looking from ?)
		* Actually, the positioning of the camera doesn't matter as it is static (even it is dynamic, the initial point can be taken as a reference that is used for positioning)
		* Yet, the agents' orientation is directly effected by it. For this, we need to extract some references from the scene in order to find its orientation according to the camera
		* ANSWER : the god damn agents themselves ! If their detection windows can be used , the orientation can be found (or atleast forced on virtual agents)
		* Directly using the detection windows wouldn't be useful, as the camera angle in the real world is untrustable. A weird angle might result in way weirder result
		* Like my initial idea, we might use the relative size-distance relationship, which uses the notion of perpective. (Homography can also contribute)
	
-After the area has been generated as a mesh and the camera is positioned accordingly, we need to generate the navigable mesh on it
	+ For discussion, use the navmesh idea from pelechano and unity's own nav mesh generator. (Together with ARA* and Unity's A*for pathfinding)
	
- The position of the sun must be found using the shadow information
		+Either use the Sun position estimation work by Gudukbay or just send a ray from the each agent's shadow to its head
	
Pedestrian Detection
- Investigate the detection algorithms used (in detail), and possible ways of improving them by other algortihms


Artificial Agents

-Better navigation models, as they stop at a certain distance to their goal
- Need to implement a proper personality model over the agents
- But we must also detect the behavor of the real pedestrians in the video (papers mentioned at the project work on that)
- By extracting both behavior and personality the real pedestrians, our system can adapt the virtual agents. If a dominant beh (or emotion) is found, it can be applied to virtual agents too
- Because there will be OCEAN on top of the system, there can also be an option which determines the
general mood of the crowd
- The agents should interact with eachother in the presence of near agents (which are also idle, which means avaliable for initiating a conversation)
- If possble, the visual identity of the crowd can be used to change the look of the virtual agents, so that they don't stand out much in the crowd

GUI

-Need to provide a menu that allows:
	- Custom video selection
	- Saving the footage that has been created (record functionality) as a playable video file
	- More control on the crowd in the sense of personality, emotion etc.
	- Abilty to load own models for agents
	- User can specify the enterence potints of the virtual agents from certain directions on the navigable mesh
	
	
